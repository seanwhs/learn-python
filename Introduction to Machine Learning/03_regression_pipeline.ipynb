{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "03_regression_pipeline.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "[![Open In Colab](https://colab.research.google.com/assets/colab-badge.svg)](https://colab.research.google.com/drive/1_sPEdqNneqh4RxVmpYkasbLe_Ku9JsG4?usp=sharing)"
      ],
      "metadata": {
        "id": "W5F2F99O8PAo"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "We work with the Concrete Compressive Strength Data Set available from UCI Machine Learning Repository. More information about this data set can be found here: https://archive.ics.uci.edu/ml/datasets/Concrete+Compressive+Strength \n",
        "\n",
        "A csv version of this data set is available through this GitHub repository: https://github.com/farhad-pourkamali/machine-learning/blob/main/Concrete_Data.csv \n",
        "\n",
        "In a nutshell, we have 1,030 samples with 8 input features and 1 output variable taking on continuous values so we have a regression problem. "
      ],
      "metadata": {
        "id": "rZF4kQb3Z3sy"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "import pandas as pd \n",
        "import matplotlib.pyplot as plt \n",
        "from sklearn.model_selection import train_test_split, KFold  \n",
        "from sklearn.pipeline import Pipeline \n",
        "from sklearn.preprocessing import PolynomialFeatures, StandardScaler\n",
        "from sklearn.linear_model import LinearRegression\n",
        "from sklearn.metrics import mean_squared_error"
      ],
      "metadata": {
        "id": "fhO5aqHUawB-"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df = pd.read_csv('https://raw.githubusercontent.com/farhad-pourkamali/machine-learning/main/Concrete_Data.csv')\n",
        "df"
      ],
      "metadata": {
        "id": "OOO8hit9a52H"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df.describe()"
      ],
      "metadata": {
        "id": "-KJNgMwUhNV6"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Data matrix X\n",
        "X = df.iloc[:,:-1].to_numpy()\n",
        "\n",
        "# Targets y \n",
        "y = df.iloc[:,-1].to_numpy()\n",
        "\n",
        "# Print types \n",
        "print(type(X), type(y))\n",
        "\n",
        "# Print sizes \n",
        "print(X.shape, y.shape)"
      ],
      "metadata": {
        "id": "Cj39xogBbUjW"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Divide the data into two parts: train and test sets "
      ],
      "metadata": {
        "id": "VyVnfEBlhc11"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "X_train, X_test, y_train, y_test = train_test_split(\n",
        "    X, y, test_size=0.3, random_state=42)"
      ],
      "metadata": {
        "id": "e9io0U4Ub1vJ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Print train and test sizes \n",
        "print(X_train.shape, X_test.shape)"
      ],
      "metadata": {
        "id": "48uq4KzhvJ1f"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(X_train[0],'\\n', y_train[0])"
      ],
      "metadata": {
        "id": "Rld2W-T1kUrF"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Case 1: Linear regression without preprocessing"
      ],
      "metadata": {
        "id": "0a73TYr8iAKz"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Train (i.e., find coefficients)\n",
        "reg1 = LinearRegression().fit(X_train, y_train)\n",
        "\n",
        "# Test (i.e., make predictions) \n",
        "y_pred1 = reg1.predict(X_test)\n",
        "\n",
        "# Evaluate (we use RMSE)\n",
        "err1 = mean_squared_error(y_test, y_pred1, squared=False)\n",
        "print(err1)"
      ],
      "metadata": {
        "id": "klLpJ3UUh_-i"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.metrics import explained_variance_score\n",
        "print(explained_variance_score(y_test, y_pred1))"
      ],
      "metadata": {
        "id": "B0uFWeGShy1e"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Case 2: Linear regression with preprocessing"
      ],
      "metadata": {
        "id": "AyFRuWvPjpND"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "scaler = StandardScaler()\n",
        "scaler.fit(X_train)"
      ],
      "metadata": {
        "id": "7x9R0JdrjoyX"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "X_train[0]"
      ],
      "metadata": {
        "id": "DjUfhsYKjOYa"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "X_train_pre = scaler.transform(X_train)"
      ],
      "metadata": {
        "id": "ax7TjwHmkQ4z"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "X_train_pre[0]"
      ],
      "metadata": {
        "id": "o4rq2mAGkh5p"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Train (i.e., find coefficients)\n",
        "reg2 = LinearRegression().fit(X_train_pre, y_train)\n",
        "\n",
        "# Test (i.e., make predictions) \n",
        "y_pred2 = reg2.predict(scaler.transform(X_test))\n",
        "\n",
        "# Evaluate (we use RMSE)\n",
        "err2 = mean_squared_error(y_test, y_pred2, squared=False)\n",
        "print(err2)"
      ],
      "metadata": {
        "id": "67vNp5_QktDP"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Case 3: Polynomial regression with preprocessing"
      ],
      "metadata": {
        "id": "-t1YZhLkmqVa"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "poly = PolynomialFeatures(2)\n",
        "X_train_pre_pol = poly.fit_transform(X_train_pre)\n",
        "print(X_train_pre.shape, X_train_pre_pol.shape)"
      ],
      "metadata": {
        "id": "iJIs6Mi5mp0n"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(X_train_pre[0], '\\n', X_train_pre_pol[0])"
      ],
      "metadata": {
        "id": "qNA4n3JuuEK4"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Train (i.e., find coefficients)\n",
        "reg3 = LinearRegression().fit(X_train_pre_pol, y_train)\n",
        "\n",
        "# Test (i.e., make predictions) \n",
        "y_pred3 = reg3.predict(poly.transform(scaler.transform(X_test)))\n",
        "\n",
        "# Evaluate (we use RMSE)\n",
        "err3 = mean_squared_error(y_test, y_pred3, squared=False)\n",
        "print(err3)"
      ],
      "metadata": {
        "id": "zmZEQvFMl5gS"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(explained_variance_score(y_test, y_pred3))"
      ],
      "metadata": {
        "id": "qpTDPoo1mW0o"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Case 4: Polynomial regression with preprocessing (nice way!)\n",
        "\n",
        "We set up a very basic *pipeline* that consists of the following sequence:\n",
        "\n",
        "* scaler: For pre-processing data, i.e., transform the data to zero mean and unit variance using the StandardScaler()\n",
        "\n",
        "* poly: For creating polynomial features using the PolynomialFeatures()\n",
        "\n",
        "* Regressor: LinearRegression(), which implements the linear regression algorithm."
      ],
      "metadata": {
        "id": "5znNQq06ue-g"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Define pipe\n",
        "pipe = Pipeline([\n",
        "('scaler', StandardScaler()),\n",
        "('poly', PolynomialFeatures(2)),\n",
        "('regressor', LinearRegression())\n",
        "])\n",
        "\n",
        "# Train \n",
        "pipe.fit(X_train, y_train)\n",
        "\n",
        "# Predict \n",
        "y_pred4 = pipe.predict(X_test)\n",
        "err4 = mean_squared_error(y_test, y_pred4, squared=False)\n",
        "print(err4)"
      ],
      "metadata": {
        "id": "hZzCzqfRuebx"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(explained_variance_score(y_test, y_pred4))"
      ],
      "metadata": {
        "id": "MHhiAnHvnnCl"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "To learn more about using *pipeline*, you can find a short tutorial at https://youtu.be/jzKSAeJpC6s"
      ],
      "metadata": {
        "id": "Ssvu2nUbwbEC"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Instead of using train/test split, we can use cross-validation (or CV for short). A k-fold CV means that:\n",
        "\n",
        "* a model is trained using (k-1) of the folds as training data;\n",
        "\n",
        "* the resulting model is validated on the remaining part of the data."
      ],
      "metadata": {
        "id": "8Smvjth9y_bM"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "kf = KFold(n_splits=3, shuffle=True, random_state=6) # shuffle the data before splitting into batches\n",
        "scores = []\n",
        "for train_index, test_index in kf.split(X):\n",
        "    X_train, X_test = X[train_index], X[test_index]\n",
        "    y_train, y_test = y[train_index], y[test_index]\n",
        "    scores.append(explained_variance_score(y_test, pipe.fit(X_train, y_train).predict(X_test)))\n",
        "\n",
        "scores = np.array(scores)\n",
        "print(scores)\n",
        "print(\"%0.2f mean score with a standard deviation of %0.2f\" % (scores.mean(), scores.std()))"
      ],
      "metadata": {
        "id": "1-jo3jx1waQq"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}