{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "## LC-4: Sequential Chain"
      ],
      "metadata": {
        "id": "gbXU_8M9Bkoc"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##**Set-up**\n",
        "\n",
        "---"
      ],
      "metadata": {
        "id": "zgz2n1OkFB6I"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "kWpPXEquEkxu",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "c694eca5-60ff-4329-b250-8687493ce021"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m267.1/267.1 kB\u001b[0m \u001b[31m3.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m812.8/812.8 kB\u001b[0m \u001b[31m16.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m75.6/75.6 kB\u001b[0m \u001b[31m5.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.9/1.9 MB\u001b[0m \u001b[31m36.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m276.8/276.8 kB\u001b[0m \u001b[31m15.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m91.7/91.7 kB\u001b[0m \u001b[31m6.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.8/1.8 MB\u001b[0m \u001b[31m32.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m49.4/49.4 kB\u001b[0m \u001b[31m1.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m77.9/77.9 kB\u001b[0m \u001b[31m1.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m58.3/58.3 kB\u001b[0m \u001b[31m1.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m53.0/53.0 kB\u001b[0m \u001b[31m2.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m144.8/144.8 kB\u001b[0m \u001b[31m952.4 kB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h"
          ]
        }
      ],
      "source": [
        "!pip install -q openai langchain langchain_openai"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import userdata"
      ],
      "metadata": {
        "id": "bptzmgBsFtBG"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## **LLMChain (Recap)**\n",
        "\n",
        "---\n",
        "\n"
      ],
      "metadata": {
        "id": "9gPKFZOBKtku"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Create a Prompt Template and use the LLMChain to extract key information from a paragraph of information!"
      ],
      "metadata": {
        "id": "wU3jG2ktKzl_"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from langchain.prompts import PromptTemplate\n",
        "from langchain_openai import OpenAI\n",
        "from langchain.chains import LLMChain"
      ],
      "metadata": {
        "id": "KeEra9Jx-DO-"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Creating the Prompt Template\n",
        "key_information = PromptTemplate(template = \"You are a scribe who summarises the key takeaways in bullet points. This is the article: {article}\", input_variables = [\"article\"])"
      ],
      "metadata": {
        "id": "yHKqyayFLBX2"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Setting Up the Chain\n",
        "llm = OpenAI(openai_api_key = userdata.get(\"OPENAI_API_KEY\"))\n",
        "llm_chain = LLMChain(prompt = key_information, llm = llm)"
      ],
      "metadata": {
        "id": "T8r4fOHWLl2R"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# From: (https://buildingblocs.sg/)\n",
        "# Parsing the Article and getting the response\n",
        "print(llm_chain.invoke(\"\"\"\n",
        "BuildingBloCS is a year-long outreach programme for students, by students, aimed at promoting more exposure to Computing and CS as a whole to Secondary School, JC and IP students.\n",
        "By students, for students.\n",
        "\n",
        "BuildingBloCS is the largest nationwide “By Student, For Student” Computing Advocacy Program, aimed at teaching students the way of the coder 💻.\n",
        "\n",
        "Multi-focused.\n",
        "\n",
        "We touch base on various topics, like AI, Cybersecurity and Software Development, to varying levels of complexity. It's truly fun for everyone!\n",
        "\n",
        "Passionate Speakers.\n",
        "\n",
        "We give opportunities to various speakers to talk about their work and interests, and we don't shy away from getting into the nitty-gritty.\n",
        "\n",
        "Events for everyone.\n",
        "\n",
        "We host events for students from Secondary Schools, JCs, Polytechnics and International Schools. All are welcome!\n",
        "\"\"\")[\"text\"])"
      ],
      "metadata": {
        "id": "4V3EmJ7-Lz1x",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "f084f288-8a59-4477-d8ba-e712581ba320"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "- BuildingBloCS is a year-long outreach programme for students, by students\n",
            "- Aims to promote more exposure to Computing and CS for Secondary School, JC, and IP students\n",
            "- Largest nationwide \"By Student, For Student\" Computing Advocacy Program\n",
            "- Multi-focused, covering topics such as AI, Cybersecurity, and Software Development\n",
            "- Fun for everyone, with varying levels of complexity\n",
            "- Features passionate speakers who talk about their work and interests\n",
            "- Events for students from Secondary Schools, JCs, Polytechnics, and International Schools\n",
            "- All are welcome to attend.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## **Sequential Chain**\n",
        "---\n"
      ],
      "metadata": {
        "id": "WaQktEwNTuu4"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "But, what if I want to use multiple of this LLM Chain function?\n",
        "\n",
        "In this case, to link a chain with another chain, we will need to use something known as a `SequentialChain` which means that the output of one chain will be the input to another chain.\n"
      ],
      "metadata": {
        "id": "CnJOj9-sUKVn"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from langchain.chains import LLMChain, SequentialChain\n",
        "# Initialise connection to the OpenAI Model\n",
        "llm = OpenAI(openai_api_key = userdata.get(\"OPENAI_API_KEY\"))\n",
        "# Creating a Prompt Template for the first LLMChain\n",
        "\n",
        "# output_key is given for us to track the result in the first chain\n"
      ],
      "metadata": {
        "id": "PImj37U0T0Zv"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Initialise another Prompt Template for the 2nd Action\n",
        "# Notice the `topics` is the output_key from the first chain -> 1st Chain Output is being passed into 2nd Chain input\n",
        "\n",
        "# Initialise another LLMChain for our 2nd Action\n"
      ],
      "metadata": {
        "id": "EXjT2Dn6Vdzp"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Use Sequential Chain to merge the two chains together with the variable overall_chain\n"
      ],
      "metadata": {
        "id": "t245nOm9Vy-e"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Use the chain and assign to be the variable response\n"
      ],
      "metadata": {
        "id": "dihgoganWP8_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(result['questions'])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "k7C_HCkbWltn",
        "outputId": "354095e4-fc93-496a-f561-d962de629ccd"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            " \n",
            "\n",
            "3. Interview with a conference or panel discussion: This type of interview would involve a more interactive and dynamic format, where the engineer would be asked questions by a moderator or panel of experts in front of a live audience. The questions could cover a wide range of topics such as the engineer's experience with implementing devops in different environments, their thoughts on the challenges and opportunities in the field, and their advice for aspiring devops professionals. This type of interview would also allow for discussions and debates on current and emerging trends in devops.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## **LC-5: Output Parsing**\n",
        "\n",
        "---\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "LqmRBOg3hnTk"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Sometimes, we would want the response of our LLM to be curated in such a way that is easily read by other readers or simply copiable to a existing project we are working on.\n",
        "\n",
        "Fret not, with LangChain it is possible for you to make the Output just like how you want it to be!\n",
        "\n",
        "Here are some examples:\n",
        "\n",
        "### **JSON**"
      ],
      "metadata": {
        "id": "ys-OPwkMhxsq"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "poem_template = \"\"\"\n",
        "You are a Singapore poet specialising in creating different poems according to the different places in Singapore.\n",
        "Imagine you are in {area}, construct a poem capturing the vibe, culture and heritage of that area.\n",
        "\n",
        "Format the output with the following keys: area, poem\n",
        "\n",
        "In the output, include the trailing ``` at the start and end of the JSON Object\n",
        "\"\"\""
      ],
      "metadata": {
        "id": "oARwz7r-hszn"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Create a prompt template from the poem_template above with the variable prompt_template\n"
      ],
      "metadata": {
        "id": "pmnEo7C6jTrs"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Use the Prompt Template with LLMChain\n"
      ],
      "metadata": {
        "id": "F0a3zqisjmkJ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Print the output of the LLM Chain\n"
      ],
      "metadata": {
        "id": "y1dLnpCNkUQT"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "###**LC-6: Structured Output Parser**\n",
        "\n",
        "---\n"
      ],
      "metadata": {
        "id": "mzjm3xIgmJSX"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### **Dependencies**"
      ],
      "metadata": {
        "id": "N0kocJg3lsrC"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from langchain.output_parsers import ResponseSchema, StructuredOutputParser\n",
        "from langchain.prompts import PromptTemplate\n",
        "from langchain_openai import ChatOpenAI"
      ],
      "metadata": {
        "id": "iHMGTTP6c5LY"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "The `ResponseSchema` class is used to define the structure of the output we want to get from the language model.\n",
        "\n",
        "Think of it as creating a form with specific fields that need to be filled out. Each `ResponseSchema` has a `name` and a `description`, which help identify what each part of the output is for.\n",
        "\n",
        "\n",
        "In the code below, there are two schemas defined: one for the 'answer' to a user's question and another for the 'source' of that answer, which should be a website URL."
      ],
      "metadata": {
        "id": "8O7iX81Rm2nf"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Create a list to categorise the different aspect of the model answer\n",
        "response_schemas = [\n",
        "    # ResponseSchema takes in a name and a description\n",
        "\n",
        "\n",
        "]\n",
        "# Output_parser would help to organise the model answer\n",
        "output_parser = StructuredOutputParser.from_response_schemas(_______)\n",
        "print(output_parser)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "roXdW_wGdai9",
        "outputId": "6be2a21f-d653-465e-ffdd-03149770306c"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "response_schemas=[ResponseSchema(name='answer', description='answer to the users question', type='string'), ResponseSchema(name='source', description='source used to answer the question', type='string')]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Format Instructions would convert the output we want to a prompt we can use in our Prompt Template\n",
        "format_instructions = output_parser.get_format_instructions()\n",
        "print(format_instructions)\n",
        "# Partial Variable here as the output that we want is already defined in the ResponseSchema\n",
        "prompt = PromptTemplate(\n",
        "    template=\"answer the users question as best as possible.\\n{format_instructions}\\n{question}\",\n",
        "    input_variables=[\"question\"],\n",
        "\n",
        "\n",
        ")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "SwtVF5BNd7ZN",
        "outputId": "2249a6dd-abcd-4b84-aa88-2398c4be3387"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "The output should be a markdown code snippet formatted in the following schema, including the leading and trailing \"```json\" and \"```\":\n",
            "\n",
            "```json\n",
            "{\n",
            "\t\"answer\": string  // answer to the users question\n",
            "\t\"source\": string  // source used to answer the question\n",
            "}\n",
            "```\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "model = ChatOpenAI(openai_api_key = userdata.get(\"OPENAI_API_KEY\"))\n",
        "chain = prompt | model | output_parser"
      ],
      "metadata": {
        "id": "M25mJIZSeK8q"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "response = chain.invoke({\"question\": \"what's the capital of france?\"})"
      ],
      "metadata": {
        "id": "I4htQ-DnfOsY"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(response)\n",
        "print(response[\"answer\"])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "KbqcxBzugIPs",
        "outputId": "2c68fa6e-90e8-4f4e-bf9b-173bf09751f6"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "{'answer': 'Paris', 'source': 'General knowledge'}\n",
            "Paris\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### **Application**"
      ],
      "metadata": {
        "id": "tBTxuEailz6f"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "In the Chinatown Poet Creator Context:"
      ],
      "metadata": {
        "id": "fKhbO_6ifQTb"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from langchain.output_parsers import ResponseSchema, StructuredOutputParser\n",
        "from langchain.prompts import PromptTemplate\n",
        "from langchain_openai import ChatOpenAI"
      ],
      "metadata": {
        "id": "s4pfgk_4g9pQ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "response_schemas = [\n",
        "    ResponseSchema(name = \"Area\", description = \"Area tha the Poet draw inspiration from\"),\n",
        "    ResponseSchema(name = \"Poem\", description = \"Poem\")\n",
        "]\n",
        "output_parser = StructuredOutputParser.from_response_schemas(response_schemas)"
      ],
      "metadata": {
        "id": "-GaYccb6hM4R"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "format_instructions = output_parser.get_format_instructions()\n",
        "prompt = PromptTemplate(template = \"Generate a Singaporean-style poem from this {area} \\n {format_instructions}\",\n",
        "                        input_variables = [\"area\"],\n",
        "                        partial_variables = {\"format_instructions\": format_instructions})\n"
      ],
      "metadata": {
        "id": "RsoZhOWKiA3h"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model = ChatOpenAI(openai_api_key = userdata.get(\"OPENAI_API_KEY\"))\n",
        "chain = prompt | model | output_parser"
      ],
      "metadata": {
        "id": "tLFHsLR4jNK1"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "response = chain.invoke({\"area\": \"Chinatown\"})"
      ],
      "metadata": {
        "id": "nsn5mtxqjUQH"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(response['Area'])"
      ],
      "metadata": {
        "id": "7WQBc3MMjc2i"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "The code can always be reused! Just ensure to change the `ResponseSchema` for any use cases that you have!"
      ],
      "metadata": {
        "id": "1HLlDAkdfTFA"
      }
    }
  ]
}