{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "## LC-4: Sequential Chain"
      ],
      "metadata": {
        "id": "o8vKW6T2B9zb"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##**Set-up**\n",
        "\n",
        "---"
      ],
      "metadata": {
        "id": "zgz2n1OkFB6I"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "kWpPXEquEkxu",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "4e7f5618-bcaf-4bc9-df0c-cd9588aa1540"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m267.1/267.1 kB\u001b[0m \u001b[31m2.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m812.8/812.8 kB\u001b[0m \u001b[31m5.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m75.6/75.6 kB\u001b[0m \u001b[31m4.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.9/1.9 MB\u001b[0m \u001b[31m11.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m276.8/276.8 kB\u001b[0m \u001b[31m12.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m91.7/91.7 kB\u001b[0m \u001b[31m5.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.8/1.8 MB\u001b[0m \u001b[31m15.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m49.4/49.4 kB\u001b[0m \u001b[31m1.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m77.9/77.9 kB\u001b[0m \u001b[31m3.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m58.3/58.3 kB\u001b[0m \u001b[31m3.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m53.0/53.0 kB\u001b[0m \u001b[31m3.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m144.8/144.8 kB\u001b[0m \u001b[31m2.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h"
          ]
        }
      ],
      "source": [
        "!pip install -q openai langchain langchain_openai"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import userdata"
      ],
      "metadata": {
        "id": "bptzmgBsFtBG"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## **LLMChain**\n",
        "\n",
        "---\n",
        "\n",
        "Remember the LLM Chain that we used from the previous lecture? It takes in a prompt template and generate a response from the LLM that we are using.\n",
        "\n",
        "Here is the code that we have used:"
      ],
      "metadata": {
        "id": "03AMWHuDGEte"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from langchain.prompts import PromptTemplate\n",
        "from langchain_openai import OpenAI\n",
        "from langchain.chains import LLMChain\n",
        "# Initalise a \"client\" to connect with the OpenAI server\n",
        "llm = OpenAI(openai_api_key = userdata.get(\"OPENAI_API_KEY\"))"
      ],
      "metadata": {
        "id": "er2xYvY1Gl2C"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Creating the Prompt Template\n",
        "template = \"You are a professional advisor working in this {field}\"\n",
        "prompt = PromptTemplate(template = template, input_variables=['field'])"
      ],
      "metadata": {
        "id": "Kk-pnVExG2Xi"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Using the LLMChain\n",
        "llm_chain = LLMChain(prompt = prompt, llm = llm)\n",
        "print(llm_chain.invoke(\"Artifical Intelligence\")[\"text\"])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "WYfHDL4bHPI1",
        "outputId": "73214d59-e2a8-4dc8-fcc3-9ddba7b6f7fa"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            " startup and you have been tasked with providing advice on the ethical considerations surrounding the development and implementation of AI technology.\n",
            "\n",
            "First and foremost, it is important to recognize that AI technology has the potential to greatly benefit society and improve our lives in various ways. However, as with any emerging technology, there are also potential ethical concerns that must be addressed to ensure responsible and ethical development and implementation of AI.\n",
            "\n",
            "One of the main ethical considerations surrounding AI is the potential bias in the data used to train AI algorithms. Since AI systems are only as good as the data they are trained on, it is crucial to ensure that the data is representative and free from bias. This requires careful selection and curation of data sets, as well as ongoing monitoring and updating to prevent bias from being perpetuated.\n",
            "\n",
            "Another important consideration is the potential impact of AI on employment and the workforce. As AI technology continues to advance, there is a concern that it may replace human workers, leading to job displacement and economic inequality. It is important for companies to consider the potential social and economic impacts of their AI technology and work towards solutions that mitigate any negative effects.\n",
            "\n",
            "Privacy and security are also major ethical concerns in the development and use of AI. With the vast amount of personal data being collected and used by AI systems, there is\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## **LLMChain (Recap)**\n",
        "\n",
        "---\n",
        "\n"
      ],
      "metadata": {
        "id": "9gPKFZOBKtku"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Create a Prompt Template and use the LLMChain to extract key information from a paragraph of information!"
      ],
      "metadata": {
        "id": "wU3jG2ktKzl_"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Creating the Prompt Template\n",
        "key_information = PromptTemplate(template = \"You are a scribe who summarises the key takeaways in bullet points. This is the article: {article}\", input_variables = [\"article\"])"
      ],
      "metadata": {
        "id": "yHKqyayFLBX2"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Setting Up the Chain\n",
        "llm = OpenAI(openai_api_key = userdata.get(\"OPENAI_API_KEY\"))\n",
        "llm_chain = LLMChain(prompt = key_information, llm = llm)"
      ],
      "metadata": {
        "id": "T8r4fOHWLl2R"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# From: (https://buildingblocs.sg/)\n",
        "# Parsing the Article and getting the response\n",
        "print(llm_chain.invoke(\"\"\"\n",
        "BuildingBloCS is a year-long outreach programme for students, by students, aimed at promoting more exposure to Computing and CS as a whole to Secondary School, JC and IP students.\n",
        "By students, for students.\n",
        "\n",
        "BuildingBloCS is the largest nationwide “By Student, For Student” Computing Advocacy Program, aimed at teaching students the way of the coder 💻.\n",
        "\n",
        "Multi-focused.\n",
        "\n",
        "We touch base on various topics, like AI, Cybersecurity and Software Development, to varying levels of complexity. It's truly fun for everyone!\n",
        "\n",
        "Passionate Speakers.\n",
        "\n",
        "We give opportunities to various speakers to talk about their work and interests, and we don't shy away from getting into the nitty-gritty.\n",
        "\n",
        "Events for everyone.\n",
        "\n",
        "We host events for students from Secondary Schools, JCs, Polytechnics and International Schools. All are welcome!\n",
        "\"\"\")[\"text\"])"
      ],
      "metadata": {
        "id": "4V3EmJ7-Lz1x"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## **Sequential Chain**\n",
        "---\n"
      ],
      "metadata": {
        "id": "WaQktEwNTuu4"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "But, what if I want to use multiple of this LLM Chain function?\n",
        "\n",
        "In this case, to link a chain with another chain, we will need to use something known as a `SequentialChain` which means that the output of one chain will be the input to another chain.\n"
      ],
      "metadata": {
        "id": "CnJOj9-sUKVn"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from langchain.chains import LLMChain, SequentialChain\n",
        "# Initialise connection to the OpenAI Model\n",
        "llm = OpenAI(openai_api_key = userdata.get(\"OPENAI_API_KEY\"))\n",
        "# Creating a Prompt Template for the first LLMChain\n",
        "prompt1 = PromptTemplate(template = \"Suggest 2 important {difficulty} interview topics for job profile {job_profile}\",\n",
        "                         input_variables = ['difficulty', 'job_profile'])\n",
        "# output_key is given for us to track the result in the first chain\n",
        "chain1 = LLMChain(llm = llm, prompt = prompt1, output_key = \"topics\")"
      ],
      "metadata": {
        "id": "PImj37U0T0Zv"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Initialise another Prompt Template for the 2nd Action\n",
        "# Notice the `topics` is the output_key from the first chain -> 1st Chain Output is being passed into 2nd Chain input\n",
        "prompt2 = PromptTemplate(template = \"For the given topics, give me 3 questions for each topic: {topics}\",\n",
        "                         input_variables = ['topics'])\n",
        "# Initialise another LLMChain for our 2nd Action\n",
        "chain2 = LLMChain(llm = llm, prompt = prompt2, output_key = \"questions\")"
      ],
      "metadata": {
        "id": "EXjT2Dn6Vdzp"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "overall_chain = SequentialChain(chains = [chain1, chain2],\n",
        "                                input_variables = ['difficulty', 'job_profile'],\n",
        "                                output_variables = ['topics', 'questions'])"
      ],
      "metadata": {
        "id": "t245nOm9Vy-e"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "result = overall_chain.invoke({\"difficulty\": 'medium', 'job_profile': 'devops engineer'})"
      ],
      "metadata": {
        "id": "dihgoganWP8_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(result[\"questions\"])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "k7C_HCkbWltn",
        "outputId": "1d479487-ced1-48b8-e171-3083d1bf2d18"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            " \n",
            "\n",
            "3. DevOps culture and mindset: This topic would explore the cultural and mindset shift required for successful adoption of DevOps practices in an organization. It would cover topics such as the importance of communication and collaboration, breaking down silos between teams, and fostering a culture of continuous learning and improvement. This topic would also discuss the challenges faced in implementing a DevOps culture and how to overcome them. \n",
            "\n",
            "1. Understanding the role and responsibilities of a DevOps Engineer:\n",
            "- What are the key responsibilities of a DevOps Engineer?\n",
            "- How does a DevOps Engineer collaborate with development and operations teams?\n",
            "- What are the main tasks involved in automation for a DevOps Engineer?\n",
            "\n",
            "2. Importance of DevOps in the current industry landscape:\n",
            "- How has the role of a DevOps Engineer evolved over the years?\n",
            "- What are the benefits of implementing DevOps practices in an organization?\n",
            "- How does DevOps contribute to the overall success of an organization?\n",
            "\n",
            "3. DevOps culture and mindset:\n",
            "- What cultural changes are necessary for successful adoption of DevOps practices?\n",
            "- How does a DevOps culture promote communication and collaboration?\n",
            "- What challenges may arise in implementing a DevOps culture and how can they be addressed?\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## **LLMMath (from documentation)**\n",
        "\n",
        "---\n"
      ],
      "metadata": {
        "id": "4YN-KmvMJmnu"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Try to learn how to use the chain from the documentation below:\n",
        "https://api.python.langchain.com/en/stable/chains/langchain.chains.llm_math.base.LLMMathChain.html#langchain.chains.llm_math.base.LLMMathChain"
      ],
      "metadata": {
        "id": "5Locy_GHRONr"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Solution\n",
        "from langchain.chains import LLMMathChain\n",
        "llm = OpenAI(openai_api_key = userdata.get(\"OPENAI_API_KEY\"))\n",
        "llm_math = LLMMathChain.from_llm(llm)\n",
        "llm_math.invoke(\"What is 5 multiplied by 5\")[\"answer\"]"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "id": "wTV4-6gvJxWv",
        "outputId": "5109ad14-7eef-4d06-fcca-57bf2286d4aa"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'Answer: 25'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 21
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## **LC-5: Output Parsing**\n",
        "\n",
        "---\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "LqmRBOg3hnTk"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Sometimes, we would want the response of our LLM to be curated in such a way that is easily read by other readers or simply copiable to a existing project we are working on.\n",
        "\n",
        "Fret not, with LangChain it is possible for you to make the Output just like how you want it to be!\n",
        "\n",
        "Here are some examples:\n",
        "\n",
        "### **JSON**"
      ],
      "metadata": {
        "id": "ys-OPwkMhxsq"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "i"
      ],
      "metadata": {
        "id": "oARwz7r-hszn"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "prompt_template = PromptTemplate(template = poem_template, input_variables = ['area'])"
      ],
      "metadata": {
        "id": "pmnEo7C6jTrs"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "chain = LLMChain(llm = llm, prompt = prompt_template)\n",
        "output = chain.invoke(\"Chinatown\")"
      ],
      "metadata": {
        "id": "F0a3zqisjmkJ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(output[\"text\"])"
      ],
      "metadata": {
        "id": "y1dLnpCNkUQT",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "f92236ce-cf1b-4dca-9910-dbf0ca9ed820"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "```\n",
            "{\n",
            "    \"area\": \"Chinatown\",\n",
            "    \"poem\": \"The streets are alive with vibrant hues\\nRed lanterns swaying, dragons on the loose\\nAromatic smells from food stalls fill the air\\nAs people gather, laughter and chatter everywhere\\nFrom the bustling markets to the temple's serene\\nChinatown, a melting pot of cultures, can be seen\\nAmidst the modern skyline, a glimpse of the past\\nPreserved in the architecture, a heritage that will last\\nFrom traditional medicine to modern fashion trends\\nChinatown, a fusion of old and new, never ends\\nThe pulse of Singapore, a symbol of diversity\\nChinatown, a place that captures the city's identity\"\n",
            "}\n",
            "```\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "###**LC-6: Structured Output Parser**\n",
        "\n",
        "---\n"
      ],
      "metadata": {
        "id": "mzjm3xIgmJSX"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### **Dependencies**"
      ],
      "metadata": {
        "id": "N0kocJg3lsrC"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from langchain.output_parsers import ResponseSchema, StructuredOutputParser\n",
        "from langchain.prompts import PromptTemplate\n",
        "from langchain_openai import ChatOpenAI"
      ],
      "metadata": {
        "id": "iHMGTTP6c5LY"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "The `ResponseSchema` class is used to define the structure of the output we want to get from the language model.\n",
        "\n",
        "Think of it as creating a form with specific fields that need to be filled out. Each `ResponseSchema` has a `name` and a `description`, which help identify what each part of the output is for.\n",
        "\n",
        "\n",
        "In the code below, there are two schemas defined: one for the 'answer' to a user's question and another for the 'source' of that answer, which should be a website URL."
      ],
      "metadata": {
        "id": "8O7iX81Rm2nf"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Create a list to categorise the different aspect of the model answer\n",
        "response_schemas = [\n",
        "    # ResponseSchema takes in a name and a description\n",
        "    ResponseSchema(name=\"answer\", description=\"answer to the user's question\"),\n",
        "    ResponseSchema(\n",
        "        name=\"source\",\n",
        "        description=\"source used to answer the user's question\",\n",
        "    ),\n",
        "]\n",
        "# Output_parser would help to organise the model answer\n",
        "output_parser = StructuredOutputParser.from_response_schemas(response_schemas)\n",
        "print(output_parser)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "roXdW_wGdai9",
        "outputId": "733b6a76-16b6-4170-abfd-a5c971606a53"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "response_schemas=[ResponseSchema(name='answer', description=\"answer to the user's question\", type='string'), ResponseSchema(name='source', description=\"source used to answer the user's question\", type='string')]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Format Instructions would convert the output we want to a prompt we can use in our Prompt Template\n",
        "format_instructions = output_parser.get_format_instructions()\n",
        "print(format_instructions)\n",
        "# Partial Variable here as the output that we want is already defined in the ResponseSchema\n",
        "prompt = PromptTemplate(\n",
        "    template=\"answer the users question as best as possible.\\n{format_instructions}\\n{question}\",\n",
        "    input_variables=[\"question\"],\n",
        "    partial_variables={\"format_instructions\": format_instructions},\n",
        ")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "SwtVF5BNd7ZN",
        "outputId": "4a9d6472-9130-4fbe-bd1a-fbbc99e40378"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "The output should be a markdown code snippet formatted in the following schema, including the leading and trailing \"```json\" and \"```\":\n",
            "\n",
            "```json\n",
            "{\n",
            "\t\"answer\": string  // answer to the user's question\n",
            "\t\"source\": string  // source used to answer the user's question\n",
            "}\n",
            "```\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "model = ChatOpenAI(openai_api_key = userdata.get(\"OPENAI_API_KEY\"))\n",
        "chain = prompt | model | output_parser"
      ],
      "metadata": {
        "id": "M25mJIZSeK8q"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "response = chain.invoke({\"question\": \"what's the capital of france?\"})"
      ],
      "metadata": {
        "id": "I4htQ-DnfOsY"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(response)\n",
        "print(response[\"answer\"])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "KbqcxBzugIPs",
        "outputId": "d53c4598-7afe-4900-ad30-af9fd8db4f26"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "{'answer': 'Paris', 'source': 'General knowledge'}\n",
            "Paris\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### **Application**"
      ],
      "metadata": {
        "id": "tBTxuEailz6f"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from langchain.output_parsers import ResponseSchema, StructuredOutputParser\n",
        "from langchain.prompts import PromptTemplate\n",
        "from langchain_openai import ChatOpenAI"
      ],
      "metadata": {
        "id": "s4pfgk_4g9pQ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "response_schemas = [\n",
        "    ResponseSchema(name = \"Area\", description = \"Area tha the Poet draw inspiration from\"),\n",
        "    ResponseSchema(name = \"Poem\", description = \"Poem\")\n",
        "]\n",
        "output_parser = StructuredOutputParser.from_response_schemas(response_schemas)"
      ],
      "metadata": {
        "id": "-GaYccb6hM4R"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "format_instructions = output_parser.get_format_instructions()\n",
        "prompt = PromptTemplate(template = \"Generate a Singaporean-style poem from this {area} \\n {format_instructions}\",\n",
        "                        input_variables = [\"area\"],\n",
        "                        partial_variables = {\"format_instructions\": format_instructions})\n"
      ],
      "metadata": {
        "id": "RsoZhOWKiA3h"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model = ChatOpenAI(openai_api_key = userdata.get(\"OPENAI_API_KEY\"))\n",
        "chain = prompt | model | output_parser"
      ],
      "metadata": {
        "id": "tLFHsLR4jNK1"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "response = chain.invoke({\"area\": \"Chinatown\"})"
      ],
      "metadata": {
        "id": "nsn5mtxqjUQH"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(response[\"Poem\"])"
      ],
      "metadata": {
        "id": "7WQBc3MMjc2i"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}